{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Char RNN 生成文本\n",
    "由于文本是由单词或者字符按照序列组成的，生成文本的任务可以利用循环神经网络进行序列建模。本教程介绍基于循环神经网络的文本生成方法，改编自github[链接](https://github.com/L1aoXingyu/Char-RNN-PyTorch)。\n",
    "\n",
    "### 训练过程\n",
    " Char RNN 文本生成可以看作多对多的序列建模任务，也就是输入一个序列，输出一个同样长度的文本预测序列。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "若输入是一个序列 \"床 前 明 月 光\"，则输出也是一个对应的序列 \"前 明 月 光 床\"。即循环神经网络的每一步输出都是预测下一步的输入字符，这样可以将文本生成任务用监督式的训练过程来实现。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面我们用 PyTorch 来具体实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "我们使用古诗来作为例子，先读取数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-18T03:28:52.315656Z",
     "start_time": "2018-02-18T03:28:52.286844Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('./dataset/poetry-small.txt', 'r') as f:\n",
    "    poetry_corpus = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-18T03:28:52.331908Z",
     "start_time": "2018-02-18T03:28:52.317790Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'寒随穷律变，春逐鸟声开。\\n初风飘带柳，晚雪间花梅。\\n碧林青旧竹，绿沼翠新苔。\\n芝田初雁去，绮树巧莺来。\\n晚霞聊自怡，初晴弥可喜。\\n日晃百花色，风动千林翠。\\n池鱼跃不同，园鸟声还异。\\n寄言博通者，知予物'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poetry_corpus[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-18T03:28:52.338277Z",
     "start_time": "2018-02-18T03:28:52.334069Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19629"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 看看字符数\n",
    "len(poetry_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "为了可视化比较方便，我们将一些其他字符替换成空格"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-18T03:28:52.353185Z",
     "start_time": "2018-02-18T03:28:52.340405Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'寒随穷律变 春逐鸟声开  初风飘带柳 晚雪间花梅  碧林青旧竹 绿沼翠新苔  芝田初雁去 绮树巧莺来  晚霞聊自怡 初晴弥可喜  日晃百花色 风动千林翠  池鱼跃不同 园鸟声还异  寄言博通者 知予物'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poetry_corpus = poetry_corpus.replace('\\n', ' ').replace('\\r', ' ').replace('，', ' ').replace('。', ' ')\n",
    "poetry_corpus[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 文本数值表示\n",
    "对于每个文字，需要将文字转换成索引数字，对所有非重复的字符，可以从 0 开始建立索引\n",
    "\n",
    "同时为了节省内存的开销，可以字频比较低的字去掉"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-18T03:28:52.642640Z",
     "start_time": "2018-02-18T03:28:52.355357Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class TextConverter(object):\n",
    "    def __init__(self, text_path, max_vocab=5000):\n",
    "        \"\"\"建立一个字符索引转换器\n",
    "        \n",
    "        Args:\n",
    "            text_path: 文本位置\n",
    "            max_vocab: 最大的单词数量\n",
    "        \"\"\"\n",
    "        \n",
    "        with open(text_path, 'r') as f:\n",
    "            text = f.read()\n",
    "        text = text.replace('\\n', ' ').replace('\\r', ' ').replace('，', ' ').replace('。', ' ')\n",
    "        # 去掉重复的字符\n",
    "        vocab = set(text)\n",
    "\n",
    "        # 如果单词总数超过最大数值，去掉频率最低的\n",
    "        vocab_count = {}\n",
    "        \n",
    "        # 计算单词出现频率并排序\n",
    "        for word in vocab:\n",
    "            vocab_count[word] = 0\n",
    "        for word in text:\n",
    "            vocab_count[word] += 1\n",
    "        vocab_count_list = []\n",
    "        for word in vocab_count:\n",
    "            vocab_count_list.append((word, vocab_count[word]))\n",
    "        vocab_count_list.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # 如果超过最大值，截取频率最低的字符\n",
    "        if len(vocab_count_list) > max_vocab:\n",
    "            vocab_count_list = vocab_count_list[:max_vocab]\n",
    "        vocab = [x[0] for x in vocab_count_list]\n",
    "        self.vocab = vocab\n",
    "\n",
    "        self.word_to_int_table = {c: i for i, c in enumerate(self.vocab)}\n",
    "        self.int_to_word_table = dict(enumerate(self.vocab))\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        return len(self.vocab) + 1\n",
    "\n",
    "    def word_to_int(self, word):\n",
    "        if word in self.word_to_int_table:\n",
    "            return self.word_to_int_table[word]\n",
    "        else:\n",
    "            return len(self.vocab)\n",
    "\n",
    "    def int_to_word(self, index):\n",
    "        if index == len(self.vocab):\n",
    "            return '<unk>'\n",
    "        elif index < len(self.vocab):\n",
    "            return self.int_to_word_table[index]\n",
    "        else:\n",
    "            raise Exception('Unknown index!')\n",
    "\n",
    "    def text_to_arr(self, text):\n",
    "        arr = []\n",
    "        for word in text:\n",
    "            arr.append(self.word_to_int(word))\n",
    "        return np.array(arr)\n",
    "\n",
    "    def arr_to_text(self, arr):\n",
    "        words = []\n",
    "        for index in arr:\n",
    "            words.append(self.int_to_word(index))\n",
    "        return \"\".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-18T03:28:53.016322Z",
     "start_time": "2018-02-18T03:28:52.645616Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "convert = TextConverter('./dataset/poetry.txt', max_vocab=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数字化表示的字符可视化为："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-18T03:28:53.025196Z",
     "start_time": "2018-02-18T03:28:53.018514Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "寒随穷律变 春逐鸟声开\n",
      "[ 40 166 358 936 565   0  10 367 108  63  78]\n"
     ]
    }
   ],
   "source": [
    "# 原始文本字符\n",
    "txt_char = poetry_corpus[:11]\n",
    "print(txt_char)\n",
    "\n",
    "# 转化成数字\n",
    "print(convert.text_to_arr(txt_char))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构造时序样本数据\n",
    "为了输入到循环神经网络中进行训练，我们需要构造时序样本的数据，因为前面我们知道，循环神经网络存在着长时依赖的问题，所以说我们不能将所有的文本作为一个序列一起输入到循环神经网络中，我们需要将整个文本分成很多很多个序列组成 batch 输入到网络中，只要我们定好每个序列的长度，那么序列个数也就被决定了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-18T03:28:53.036447Z",
     "start_time": "2018-02-18T03:28:53.027222Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "981\n"
     ]
    }
   ],
   "source": [
    "n_step = 20\n",
    "\n",
    "# 总的序列个数\n",
    "num_seq = int(len(poetry_corpus) / n_step)\n",
    "\n",
    "# 去掉最后不足一个序列长度的部分\n",
    "text = poetry_corpus[:num_seq*n_step]\n",
    "\n",
    "print(num_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接着我们将序列中所有的文字转换成数字表示，重新排列成 (num_seq x n_step) 的矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-18T03:28:53.258155Z",
     "start_time": "2018-02-18T03:28:53.038479Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-18T03:28:53.921749Z",
     "start_time": "2018-02-18T03:28:53.260507Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([981, 20])\n",
      "tensor([ 40, 166, 358, 936, 565,   0,  10, 367, 108,  63,  78,   0,   0, 150,\n",
      "          4, 444, 284, 182,   0, 131], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "arr = convert.text_to_arr(text)\n",
    "arr = arr.reshape((num_seq, -1))\n",
    "arr = torch.from_numpy(arr)\n",
    "\n",
    "print(arr.shape)\n",
    "print(arr[0, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "据此，我们可以构建 PyTorch 中的数据读取来训练网络，这里我们将输入最后一个字符的预测输出 label 设为输入中的第一个字符，也就是\"床前明月光\"的输出是\"前明月光床\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-18T03:28:53.945768Z",
     "start_time": "2018-02-18T03:28:53.925488Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TextDataset(object):\n",
    "    def __init__(self, arr):\n",
    "        self.arr = arr\n",
    "        \n",
    "    def __getitem__(self, item):\n",
    "        x = self.arr[item, :]\n",
    "        \n",
    "        # 构造 label\n",
    "        y = torch.zeros(x.shape)\n",
    "        # 将输入的第一个字符作为最后一个输入的 label\n",
    "        y[:-1], y[-1] = x[1:], x[0]\n",
    "        return x, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.arr.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-18T03:28:53.950296Z",
     "start_time": "2018-02-18T03:28:53.947697Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_set = TextDataset(arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以取出其中一个数据集查看一下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-18T03:28:53.957705Z",
     "start_time": "2018-02-18T03:28:53.952232Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "寒随穷律变 春逐鸟声开  初风飘带柳 晚\n",
      "随穷律变 春逐鸟声开  初风飘带柳 晚寒\n"
     ]
    }
   ],
   "source": [
    "x, y = train_set[0]\n",
    "print(convert.arr_to_text(x.numpy()))\n",
    "print(convert.arr_to_text(y.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 建立模型\n",
    "模型可以定义成非常简单的三层，第一层是词嵌入，第二层是 RNN 层，因为最后是一个分类问题，所以第三层是线性层，最后输出预测的字符。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-18T03:28:54.022455Z",
     "start_time": "2018-02-18T03:28:53.959687Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "use_gpu = False\n",
    "\n",
    "class CharRNN(nn.Module):\n",
    "    def __init__(self, num_classes, embed_dim, hidden_size, \n",
    "                 num_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.word_to_vec = nn.Embedding(num_classes, embed_dim)\n",
    "        self.rnn = nn.GRU(embed_dim, hidden_size, num_layers, dropout=dropout)\n",
    "        self.project = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x, hs=None):\n",
    "        batch = x.shape[0]\n",
    "        if hs is None:\n",
    "            hs = Variable(\n",
    "                torch.zeros(self.num_layers, batch, self.hidden_size))\n",
    "            if use_gpu:\n",
    "                hs = hs.cuda()\n",
    "        word_embed = self.word_to_vec(x)  # (batch, len, embed)\n",
    "        word_embed = word_embed.permute(1, 0, 2)  # (len, batch, embed)\n",
    "        out, h0 = self.rnn(word_embed, hs)  # (len, batch, hidden)\n",
    "        le, mb, hd = out.shape\n",
    "        out = out.view(le * mb, hd)\n",
    "        out = self.project(out)\n",
    "        out = out.view(le, mb, -1)\n",
    "        out = out.permute(1, 0, 2).contiguous()  # (batch, len, hidden)\n",
    "        return out.view(-1, out.shape[2]), h0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练模型\n",
    "在训练模型的时候，将预测可能出现的下一个字符（或单词）看作一个分类任务，所以可以使用交叉熵作为 loss 函数，在语言模型中，我们通常使用一个新的指标来评估结果，这个指标叫做困惑度(perplexity)，其取值范围就是 $[1, +\\infty]$，也是越小越好。困惑度可以理解为平均分支系数（average branching factor），即模型预测下一个字符（或单词）时的平均可选择的字符（或单词）数量。\n",
    "\n",
    "另外，我们前面讲过 RNN 存在着梯度爆炸的问题，所以我们需要进行梯度裁剪，在 pytorch 中使用 `torch.nn.utils.clip_grad_norm` 实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-18T03:28:54.030508Z",
     "start_time": "2018-02-18T03:28:54.024511Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 128\n",
    "train_data = DataLoader(train_set, batch_size, False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-18T03:28:59.955521Z",
     "start_time": "2018-02-18T03:28:54.032512Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mxtorch.trainer import ScheduledOptim\n",
    "\n",
    "model = CharRNN(convert.vocab_size, 512, 512, 2, 0.5)\n",
    "if use_gpu:\n",
    "    model = model.cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "basic_optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "optimizer = ScheduledOptim(basic_optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-18T03:31:48.754799Z",
     "start_time": "2018-02-18T03:28:59.957657Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_31352\\3564348679.py:22: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(model.parameters(), 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, perplexity is: 1783.279, lr:1.0e-03\n",
      "epoch: 2, perplexity is: 513.238, lr:1.0e-03\n",
      "epoch: 3, perplexity is: 431.813, lr:1.0e-03\n",
      "epoch: 4, perplexity is: 394.713, lr:1.0e-03\n",
      "epoch: 5, perplexity is: 374.503, lr:1.0e-03\n",
      "epoch: 6, perplexity is: 355.266, lr:1.0e-03\n",
      "epoch: 7, perplexity is: 329.485, lr:1.0e-03\n",
      "epoch: 8, perplexity is: 295.102, lr:1.0e-03\n",
      "epoch: 9, perplexity is: 265.187, lr:1.0e-03\n",
      "epoch: 10, perplexity is: 247.860, lr:1.0e-03\n",
      "epoch: 11, perplexity is: 233.911, lr:1.0e-03\n",
      "epoch: 12, perplexity is: 221.790, lr:1.0e-03\n",
      "epoch: 13, perplexity is: 212.400, lr:1.0e-03\n",
      "epoch: 14, perplexity is: 202.599, lr:1.0e-03\n",
      "epoch: 15, perplexity is: 194.047, lr:1.0e-03\n",
      "epoch: 16, perplexity is: 185.602, lr:1.0e-03\n",
      "epoch: 17, perplexity is: 181.484, lr:1.0e-03\n",
      "epoch: 18, perplexity is: 177.257, lr:1.0e-03\n",
      "epoch: 19, perplexity is: 173.195, lr:1.0e-03\n",
      "epoch: 20, perplexity is: 146.599, lr:1.0e-03\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "for e in range(epochs):\n",
    "    train_loss = 0\n",
    "    for data in train_data:\n",
    "        x, y = data\n",
    "        \n",
    "        y = y.long()\n",
    "        if use_gpu:\n",
    "            x = x.cuda()\n",
    "            y = y.cuda()\n",
    "        x, y = Variable(x), Variable(y)\n",
    "        x=x.to(device,dtype=torch.long)\n",
    "\n",
    "        # Forward.\n",
    "        score, _ = model(x)\n",
    "        loss = criterion(score, y.view(-1))\n",
    "\n",
    "        # Backward.\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # Clip gradient.\n",
    "        nn.utils.clip_grad_norm(model.parameters(), 5)\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "    print('epoch: {}, perplexity is: {:.3f}, lr:{:.1e}'.format(e+1, np.exp(train_loss / len(train_data)), optimizer.lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练完模型之后，下面我们就可以开始生成文本了。\n",
    "\n",
    "### 生成文本\n",
    "生成文本的过程非常简单，前面已将讲过了，给定了开始的字符，然后不断向后生成字符，将生成的字符作为新的输入再传入网络。\n",
    "\n",
    "这里需要注意的是，为了增加更多的随机性，我们会在预测的概率最高的前五个里面依据他们的概率来进行随机选择。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-18T03:31:48.770181Z",
     "start_time": "2018-02-18T03:31:48.758123Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pick_top_n(preds, top_n=5):\n",
    "    top_pred_prob, top_pred_label = torch.topk(preds, top_n, 1)\n",
    "    top_pred_prob /= torch.sum(top_pred_prob)\n",
    "    top_pred_prob = top_pred_prob.squeeze(0).cpu().numpy()\n",
    "    top_pred_label = top_pred_label.squeeze(0).cpu().numpy()\n",
    "    c = np.random.choice(top_pred_label, size=1, p=top_pred_prob)\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-18T03:31:48.860330Z",
     "start_time": "2018-02-18T03:31:48.772317Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate text is: 一朝春夏改  一花不重春地长风长春风时   日日花不中   日云花风风\n"
     ]
    }
   ],
   "source": [
    "begin = '一朝春夏改'\n",
    "text_len = 30\n",
    "\n",
    "model = model.eval()\n",
    "samples = [convert.word_to_int(c) for c in begin]\n",
    "input_txt = torch.LongTensor(samples)[None]\n",
    "if use_gpu:\n",
    "    input_txt = input_txt.cuda()\n",
    "input_txt = Variable(input_txt)\n",
    "_, init_state = model(input_txt)\n",
    "result = samples\n",
    "model_input = input_txt[:, -1][:, None]\n",
    "for i in range(text_len):\n",
    "    out, init_state = model(model_input, init_state)\n",
    "    pred = pick_top_n(out.data)\n",
    "    model_input = Variable(torch.LongTensor(pred))[None]\n",
    "    if use_gpu:\n",
    "        model_input = model_input.cuda()\n",
    "    result.append(pred[0])\n",
    "text = convert.arr_to_text(result)\n",
    "print('Generate text is: {}'.format(text))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
